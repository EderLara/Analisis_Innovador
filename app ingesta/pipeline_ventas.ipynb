{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e2c3d6-6da1-44a6-a72e-bf6c1ade5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas sqlalchemy mysql mysqlclient psycopg2-binary pyodbc oracledb mariadb mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363c87f8-46d3-4133-b964-7a6905e0e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de módulos añadida: C:\\Users\\edela\\Documents\\Análisi Innovador\\Taller ingesta Repo\\Analisis_Innovador\\app ingesta\\etl_modules\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 1. CONFIGURACIÓN DE RUTAS Y MÓDULOS\n",
    "# ==========================================================\n",
    "import sys\n",
    "import os\n",
    "# import importlib \n",
    "# import extractors.extractors\n",
    "# importlib.reload(extractors.extractors)\n",
    "\n",
    "# Obtener la ruta de la carpeta que contiene el notebook\n",
    "notebook_dir = os.getcwd() \n",
    "\n",
    "# Añadir la carpeta 'etl_modules' a la ruta de búsqueda de Python (sys.path)\n",
    "# Esto permite importar el contenido de 'config', 'extractors', etc., directamente.\n",
    "sys.path.append(os.path.join(notebook_dir, 'etl_modules'))\n",
    "\n",
    "print(f\"Ruta de módulos añadida: {os.path.join(notebook_dir, 'etl_modules')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4129b3-3932-4cd8-8dbb-51e9256b34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2. IMPORTACIÓN DE CLASES Y FUNCIONES\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Ahora las importaciones son más limpias y reflejan la estructura de carpetas\n",
    "from extractors.extractors import (\n",
    "    MySqlSourceExtractor, OracleExtractor, PostgresExtractor, \n",
    "    SqlServerExtractor, MariaDBExtractor\n",
    ")\n",
    "from transformers.transformers import standardize_columns, clean_and_cast_data\n",
    "from loaders.loaders import SQLiteLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2710b014-5e7f-45dc-bb36-ebb53c480791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 3. LÓGICA DEL PIPELINE ETL\n",
    "# ==========================================================\n",
    "import importlib \n",
    "import extractors.extractors\n",
    "importlib.reload(extractors.extractors)\n",
    "\n",
    "# Definición de la query genérica para ventas\n",
    "VENTAS_QUERY = \"SELECT * FROM venta_data\" \n",
    "\n",
    "# Lista de extractores (DIP)\n",
    "extractor_map = {\n",
    "    \"MySQL_Source\": MySqlSourceExtractor(),\n",
    "    \"Oracle_Source\": OracleExtractor(),\n",
    "    \"Postgres_Source\": PostgresExtractor(),\n",
    "    \"SQLServer_Source\": SqlServerExtractor(),\n",
    "    \"MariaDB_Source\": MariaDBExtractor()\n",
    "}\n",
    "\n",
    "all_consolidated_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c326ac45-5bb0-4287-aa79-302c2f931343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIO DEL PIPELINE DE VENTAS (E X T R A C C I Ó N) ---\n",
      "\n",
      "[PROCESANDO FUENTE: MySQL_Source]\n",
      "-> Extrayendo datos desde: mysql\n",
      "-> Extracción exitosa. Filas: 10000\n",
      "-> Columnas estandarizadas para MySQL_Source.\n",
      "-> DataFrame de MySQL_Source listo para consolidación.\n",
      "\n",
      "[PROCESANDO FUENTE: Oracle_Source]\n",
      "-> Extrayendo datos desde: oracle\n",
      "-> Extracción exitosa. Filas: 10000\n",
      "-> Columnas estandarizadas para Oracle_Source.\n",
      "-> DataFrame de Oracle_Source listo para consolidación.\n",
      "\n",
      "[PROCESANDO FUENTE: Postgres_Source]\n",
      "-> Extrayendo datos desde: postgresql\n",
      "-> Extracción exitosa. Filas: 10000\n",
      "-> Columnas estandarizadas para Postgres_Source.\n",
      "-> DataFrame de Postgres_Source listo para consolidación.\n",
      "\n",
      "[PROCESANDO FUENTE: SQLServer_Source]\n",
      "-> Extrayendo datos desde: mssql\n",
      "ERROR de extracción en mssql: (pyodbc.OperationalError) ('08001', '[08001] [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Error no recuperable durante una búsqueda en base de datos.\\r\\n (11003) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 17 for SQL Server]A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (11003)')\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "\n",
      "[PROCESANDO FUENTE: MariaDB_Source]\n",
      "-> Extrayendo datos desde: mariadb\n",
      "-> Extracción exitosa. Filas: 10000\n",
      "-> Columnas estandarizadas para MariaDB_Source.\n",
      "-> DataFrame de MariaDB_Source listo para consolidación.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 4. PROCESAMIENTO ETL ITERATIVO\n",
    "# ==========================================================\n",
    "print(\"--- INICIO DEL PIPELINE DE VENTAS (E X T R A C C I Ó N) ---\")\n",
    "\n",
    "for source_name, extractor in extractor_map.items():\n",
    "    print(f\"\\n[PROCESANDO FUENTE: {source_name}]\")\n",
    "    \n",
    "    # E: Extracción\n",
    "    df_raw = extractor.fetch_data(VENTAS_QUERY)\n",
    "    \n",
    "    if df_raw.empty:\n",
    "        continue\n",
    "\n",
    "    # T: Transformación y Estandarización\n",
    "    df_transformed = standardize_columns(df_raw.copy(), source_name)\n",
    "    df_cleaned = clean_and_cast_data(df_transformed)\n",
    "    \n",
    "    # Asignar la fuente para auditoría\n",
    "    df_cleaned['source_db'] = source_name\n",
    "    \n",
    "    # Almacenar para consolidación\n",
    "    all_consolidated_dfs.append(df_cleaned)\n",
    "    print(f\"-> DataFrame de {source_name} listo para consolidación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f97f705-2856-4568-a198-18ccaeec6d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIO DE CONSOLIDACIÓN ---\n",
      "Total de registros a cargar: 40000\n",
      "            productos  cantidad  valor_individual     forma_pago  \\\n",
      "0          Rosquillas         4           1235.25  Transferencia   \n",
      "1        Arroz Blanco         3           2129.40  Transferencia   \n",
      "2           Salchicha         1           2018.40  Transferencia   \n",
      "3  Detergente Líquido         4           1944.80  Transferencia   \n",
      "4     Harina de Trigo         4           1816.50  Transferencia   \n",
      "\n",
      "         clientes  tiendas     vendedores     source_db  \n",
      "0      Juan Pérez  Central       Ana Soto  MySQL_Source  \n",
      "1  Valeria Moreno  Central    Carlos Ruiz  MySQL_Source  \n",
      "2   Marta Sánchez  Central  Ricardo Gómez  MySQL_Source  \n",
      "3       Ana Gómez  Central       Ana Soto  MySQL_Source  \n",
      "4    Miguel Pardo  Central  Ricardo Gómez  MySQL_Source  \n",
      "\n",
      "--- INICIO DE CARGA (L O A D) EN SQLITE ---\n",
      "-> Iniciando carga de 40000 registros a SQLite en tabla 'ventas_consolidado'...\n",
      "-> Carga finalizada exitosamente en SQLite.\n",
      "\n",
      ">>> PIPELINE ETL FINALIZADO EXITOSAMENTE <<<\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5. CONSOLIDACIÓN Y CARGA\n",
    "# ==========================================================\n",
    "\n",
    "if not all_consolidated_dfs:\n",
    "    print(\"\\nFATAL: No se extrajeron datos de ninguna fuente. Terminando.\")\n",
    "else:\n",
    "    print(\"\\n--- INICIO DE CONSOLIDACIÓN ---\")\n",
    "    df_final = pd.concat(all_consolidated_dfs, ignore_index=True, sort=False)\n",
    "    \n",
    "    # T: Transformación final\n",
    "    df_final.fillna(value={'amount': 0.0, 'customer_id': 'N/A'}, inplace=True)\n",
    "    \n",
    "    print(f\"Total de registros a cargar: {len(df_final)}\")\n",
    "    print(df_final.head())\n",
    "    \n",
    "    # L: Carga (Usando el nuevo SQLite Loader)\n",
    "    print(\"\\n--- INICIO DE CARGA (L O A D) EN SQLITE ---\")\n",
    "    sqlite_loader = SQLiteLoader() # CAMBIO: Usar el nuevo Loader\n",
    "    sqlite_loader.load_data(df_final, table_name=\"ventas_consolidado\")\n",
    "    \n",
    "    print(\"\\n>>> PIPELINE ETL FINALIZADO EXITOSAMENTE <<<\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
